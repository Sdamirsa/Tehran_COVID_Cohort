{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "from typing import final\n",
    "from openai import  AsyncOpenAI\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "# OPENAI - TEXT ---------------------------------------------\n",
    "async def OpenAI_QA_FunctionCall_general(message_content:str, model_name: str, openai_api, \n",
    "                                         model_tempreature:float=0, model_max_tokens:int=512,\n",
    "                                         request_timeout:int=30,\n",
    "                                         use_seed: bool=False):\n",
    "    \"\"\"\n",
    "    Answer a medical question using the OPENAI language model asynchronously.\n",
    "\n",
    "    Args:\n",
    "        question (str): The medical question.\n",
    "        choices (str): A string containing answer choices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the best answer, certainty, and rationale.\n",
    "               If an error occurs, all values will be None.\n",
    "               \n",
    "    Notes: 2024027version: The token counter was added and returned. The timeout was added. The function will print the model parameters and prompts on the first run.\n",
    "    \"\"\"\n",
    "    Experiment_detail={}\n",
    "    Experiment_detail['overall_prompt'] = message_content\n",
    "    Experiment_detail['model_temperature'] = model_tempreature \n",
    "    Experiment_detail['model_max_tokens'] = model_max_tokens\n",
    "    Experiment_detail['seed']= 123 if use_seed else 'None'\n",
    "\n",
    "    \n",
    "    if not hasattr(OpenAI_QA_FunctionCall_general, 'has_run_before') or not OpenAI_QA_FunctionCall_general.has_run_before:\n",
    "        print(f\"This is the first time you run this function. The current parameters are: {str(Experiment_detail)}\")\n",
    "        OpenAI_QA_FunctionCall_general.has_run_before = True\n",
    "\n",
    "    try:\n",
    "    \n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"structuring_output\",\n",
    "                    \"description\": \"Predict the prognosis of a patient admitted with COVID-19.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"Outcome\": {\"type\": \"string\",  \n",
    "                                                \"description\": \"The outcome of the patient during the admission. Possible values are 'survive' and 'die'.\",\n",
    "                                                \"enum\": [\"die\", \"survive\"]},\n",
    "                        },\n",
    "                        \"required\": [\"Outcome\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"\"\"\n",
    "             {message_content}\n",
    "             \"\"\"  }]\n",
    "        \n",
    "        client = AsyncOpenAI(api_key=openai_api)\n",
    "\n",
    "        if use_seed:\n",
    "            seed_value = 123\n",
    "        else:\n",
    "            seed_value = None\n",
    "            \n",
    "        start_time = time.time()\n",
    "        response  = await client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,  \n",
    "            max_tokens=model_max_tokens,\n",
    "            temperature=model_tempreature,\n",
    "            tools= tools,\n",
    "            logprobs=False,\n",
    "            timeout=request_timeout, \n",
    "            tool_choice= {\"type\": \"function\", \"function\": {\"name\": \"structuring_output\"}},\n",
    "            seed=seed_value\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        Experiment_detail[\"execution_time\"]=execution_time\n",
    "               \n",
    "        try:\n",
    "            correct_answer_dic=json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "            structure_answer=correct_answer_dic[\"Outcome\"]\n",
    "        except Exception as ee:\n",
    "            structure_answer=f'ERROR in extracting selected answer: {ee}'\n",
    "    except Exception as e:\n",
    "        Experiment_detail=response=structure_answer=f'ERROR in response generation: {e}'\n",
    "\n",
    "    return Experiment_detail, response, structure_answer\n",
    "\n",
    "def check_answer_correctness(answer, truth, reporterror_index=None,reporterror_name=None):\n",
    "    if reporterror_index is None:\n",
    "        reporterror_index=r'(IDK the index)'\n",
    "    if reporterror_name is None:\n",
    "        reporterror_name =r'(IDK the llm name)'\n",
    "        \n",
    "    norm_answer = str(answer).strip().upper() if answer else ''\n",
    "    norm_truth = str(truth).strip().upper() if truth else ''\n",
    "\n",
    "\n",
    "    # Check correctness\n",
    "    if 'ERROR' in answer:\n",
    "        # for cases that extraction of answer caused error\n",
    "        return 'ERROR'\n",
    "    \n",
    "    if norm_answer and norm_truth:\n",
    "        return 'correct' if norm_answer == norm_truth else 'incorrect'\n",
    "    else:\n",
    "        return 'ERROR: answer or truth invalid'\n",
    "\n",
    "def save_and_open_excel(df, excel_output_path, open_at_end):\n",
    "    try:\n",
    "        df.to_excel(excel_output_path, index=False)\n",
    "        print(f'Saved the excel file at {excel_output_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the file to excel: {e}\")\n",
    "        return df\n",
    "\n",
    "    if os.path.exists(excel_output_path) and open_at_end is True:\n",
    "        try:\n",
    "            os.startfile(excel_output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening excel: {e}\")\n",
    "            return df\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "async def handle_llm_response_functioncall_general(excel_file_path: str,  messsage_content_column:str, llm_list: list, openai_api, \n",
    "                                                   ground_truth_column:str = None,\n",
    "                                                    open_at_end:bool=False,\n",
    "                                                    number_of_task_to_save:int=15, add_delay_sec:int=1,\n",
    "                                                    request_timeout:int=15,\n",
    "                                                    model_tempreature=0, model_max_tokens=512, max_token_plus_input=False,\n",
    "                                                    use_seed: bool=False\n",
    "                                                    ):\n",
    "    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Read excel\n",
    "        df = pd.read_excel(excel_file_path,)\n",
    "        \n",
    "        async def process_row(row, llm_name, idx, model_max_tokens, max_token_plus_input,):\n",
    "            if max_token_plus_input:\n",
    "                input_token_count= row['input_token_count']\n",
    "                \n",
    "                max_token=input_token_count+model_max_tokens\n",
    "                max_token=int(max_token)\n",
    "            else:\n",
    "                max_token=int(model_max_tokens)\n",
    "                \n",
    "            Experiment_detail= response=structure_answer= correctness = None\n",
    "            Experiment_detail, response, structure_answer = await OpenAI_QA_FunctionCall_general(message_content=row[messsage_content_column], model_name=llm_name, openai_api=openai_api,request_timeout=request_timeout, \n",
    "                                                                                                model_tempreature=model_tempreature, model_max_tokens=max_token, \n",
    "                                                                                                    use_seed=use_seed)\n",
    "            correctness = check_answer_correctness(truth=row[ground_truth_column], answer=structure_answer)\n",
    "            return idx, Experiment_detail, response, structure_answer, correctness\n",
    "    \n",
    "        # Loop through llm list\n",
    "        for llm in llm_list:\n",
    "            # Create a column for each llm (to store response) if it doesn't exist\n",
    "            if llm not in df.columns:\n",
    "                df[llm] = ''\n",
    "            \n",
    "            tasks = []\n",
    "            \n",
    "            i=0\n",
    "            max_index = df.index.max()\n",
    "            for index, row in df.iterrows():\n",
    "                if row[llm] != 'EXTRACTED':\n",
    "                    \n",
    "\n",
    "                    task = asyncio.create_task(process_row(row, llm, idx=index,model_max_tokens=model_max_tokens,max_token_plus_input=max_token_plus_input))\n",
    "                    tasks.append(task)\n",
    "                    i+=1\n",
    "                    \n",
    "                    #saving output after finishing number_of_task_to_save tasks    \n",
    "                    if i==number_of_task_to_save or index == max_index: \n",
    "                        results = await asyncio.gather(*tasks)\n",
    "                        \n",
    "                        for result in results:\n",
    "                            if result:  # Ensure result is not None or handle as needed\n",
    "\n",
    "                                idx, Experiment_detail, response, structure_answer, correctness = result\n",
    "                                # Update the DataFrame based on the result\n",
    "                                df.at[idx, llm] = 'EXTRACTED'\n",
    "                                df.at[idx, f'{llm}_rawresponse'] = str(response) \n",
    "                                df.at[idx, f'{llm}_outcome'] = str(structure_answer)\n",
    "                                df.at[idx, f'{llm}_Experiment_detail'] = str(Experiment_detail)\n",
    "                                df.at[idx, f'{llm}_correctness'] = correctness\n",
    "                                \n",
    "                                print(f\"idx: {idx}, outcome: {structure_answer}\")\n",
    "                                \n",
    "                        #save draft\n",
    "                        try:\n",
    "                            df.to_excel(excel_file_path)\n",
    "                            print(f\"Draft excel file saved at {excel_file_path}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in saving temporary excel. Error:   {e}\")\n",
    "                            continue    \n",
    "                            \n",
    "                        \n",
    "                        #reset for continue\n",
    "                        i=0\n",
    "                        tasks = []\n",
    "                        print('sleep like a baby')\n",
    "                        await asyncio.sleep(add_delay_sec)\n",
    "            \n",
    "    \n",
    "    except KeyboardInterrupt or asyncio.CancelledError:\n",
    "        print(\"Operation interrupted. Cleaning up...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occured in the handler: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        df = save_and_open_excel(df, excel_file_path, open_at_end)\n",
    "        \n",
    "        \n",
    "        # reset the model experiemnt \n",
    "        OpenAI_QA_FunctionCall_general.has_run_before = False\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path=r\"C:\\Users\\LEGION\\Desktop\\Code4ghaffar\\zero_shot_by_hand.xlsx\"\n",
    "messsage_content_column=\"patient medical hidtory\"\n",
    "llm_list=[\n",
    "    #\"gpt-4o-2024-05-13\", #\tUp to Oct 2023   --> Gpt-4o\n",
    "    #\"gpt-4-turbo-2024-04-09\", #\tUp to Dec 2023 --> Gpt-4turbo\n",
    "    \"gpt-4-0613\", #\tUp to Sep 2021 --> Gpt-4\n",
    "    #\"gpt-3.5-turbo-0125\" #\tUp to September 2021 --> Gpt-3.5 \n",
    "    ]\n",
    "openai_api=os.getenv(\"OPENAI_API_KEY\")\n",
    "ground_truth_column=\"Inhospital Mortalit(TRUE)\"\n",
    "open_at_end=True\n",
    "number_of_task_to_save=10\n",
    "add_delay_sec=5\n",
    "request_timeout=30\n",
    "\n",
    "model_tempreature=1\n",
    "model_max_tokens=1024\n",
    "max_token_plus_input=False\n",
    "use_seed=True\n",
    "\n",
    "\n",
    "final_df = await handle_llm_response_functioncall_general(excel_file_path=excel_file_path,  messsage_content_column=messsage_content_column, llm_list=llm_list, openai_api=openai_api, \n",
    "                                                    ground_truth_column=ground_truth_column,\n",
    "                                                    open_at_end=open_at_end,\n",
    "                                                    number_of_task_to_save=number_of_task_to_save, add_delay_sec=add_delay_sec,\n",
    "                                                    request_timeout=request_timeout,\n",
    "                                                    \n",
    "                                                    model_tempreature=model_tempreature, model_max_tokens=model_max_tokens, max_token_plus_input=max_token_plus_input,\n",
    "                                                    use_seed=use_seed,\n",
    "                                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "from typing import final\n",
    "from openai import  OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# OPENAI - TEXT ---------------------------------------------\n",
    "def LMstudio_playing_openai(message_content,\n",
    "                                         overall_prompt:str=\"\",\n",
    "                                         model_tempreature:float=0, model_max_tokens:int=512,\n",
    "                                         request_timeout:int=30):\n",
    "    \"\"\"\n",
    "    Answer a medical question using the OPENAI language model asynchronously.\n",
    "\n",
    "    Args:\n",
    "        question (str): The medical question.\n",
    "        choices (str): A string containing answer choices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the best answer, certainty, and rationale.\n",
    "               If an error occurs, all values will be None.\n",
    "               \n",
    "    Notes: 2024027version: The token counter was added and returned. The timeout was added. The function will print the model parameters and prompts on the first run.\n",
    "    \"\"\"\n",
    "    Experiment_detail={}\n",
    "    Experiment_detail['overall_prompt'] = overall_prompt\n",
    "    Experiment_detail['model_temperature'] = model_tempreature \n",
    "    Experiment_detail['model_max_tokens'] = model_max_tokens\n",
    "\n",
    "    \n",
    "    if not hasattr(LMstudio_playing_openai, 'has_run_before') or not LMstudio_playing_openai.has_run_before:\n",
    "        print(f\"This is the first time you run this function. The current parameters are: {str(Experiment_detail)}\")\n",
    "        LMstudio_playing_openai.has_run_before = True\n",
    "        Experiment_detail['GPU Offload'] = input('First run: What is the number of GPU Offload layers')\n",
    "        Experiment_detail['CPU thread'] = input('First run: What is the number of CPU Threads')\n",
    "\n",
    "    try:\n",
    "        LMstudio_client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        response  =  LMstudio_client.chat.completions.create(\n",
    "            model=\"local-model\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \" \"},\n",
    "                {\"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                {overall_prompt}\n",
    "                {message_content}\n",
    "                \"\"\"\n",
    "                }\n",
    "                ],\n",
    "            max_tokens=model_max_tokens,\n",
    "            temperature=model_tempreature,\n",
    "            timeout=request_timeout\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        Experiment_detail[\"execution_time\"]=execution_time\n",
    "        \n",
    "                \n",
    "        raw_output=response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f'Error in LMstudio_playing_openai Generation: {e}')\n",
    "        Experiment_detail=response=f'Error in LMstudio_playing_openai Generation: {e}'\n",
    "        raw_output=None\n",
    "    finally:\n",
    "        return Experiment_detail, response, raw_output\n",
    "    \n",
    "\n",
    "def check_answer_correctness(answer, truth, reporterror_index=None,reporterror_name=None):\n",
    "    if reporterror_index is None:\n",
    "        reporterror_index=r'(IDK the index)'\n",
    "    if reporterror_name is None:\n",
    "        reporterror_name =r'(IDK the llm name)'\n",
    "        \n",
    "    norm_answer = str(answer).strip().upper() if answer else ''\n",
    "    norm_truth = str(truth).strip().upper() if truth else ''\n",
    "\n",
    "\n",
    "    # Check correctness\n",
    "    if 'ERROR' in answer:\n",
    "        # for cases that extraction of answer caused error\n",
    "        return 'ERROR'\n",
    "    \n",
    "    if norm_answer and norm_truth:\n",
    "        return 'correct' if norm_answer == norm_truth else 'incorrect'\n",
    "    else:\n",
    "        return 'ERROR: answer or truth invalid'\n",
    "\n",
    "def save_and_open_excel(df, excel_output_path, open_at_end):\n",
    "    try:\n",
    "        df.to_excel(excel_output_path, index=False)\n",
    "        print(f'Saved the excel file at {excel_output_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the file to excel: {e}\")\n",
    "        return df\n",
    "\n",
    "    if os.path.exists(excel_output_path) and open_at_end is True:\n",
    "        try:\n",
    "            os.startfile(excel_output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening excel: {e}\")\n",
    "            return df\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_LM_studio_general(excel_file_path: str,  messsage_content_column:str, llm: str, openai_api, \n",
    "                                                   ground_truth_column:str = None,\n",
    "                                                    open_at_end:bool=False,\n",
    "                                                    number_of_task_to_save:int=15, \n",
    "                                                    request_timeout:int=15,\n",
    "                                                    model_tempreature=0, model_max_tokens=512, max_token_plus_input=False,\n",
    "                                                    use_seed: bool=False\n",
    "                                                    ):\n",
    "    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Read excel\n",
    "        df = pd.read_excel(excel_file_path,)\n",
    "        \n",
    "        def process_row(row,  idx, model_max_tokens, max_token_plus_input,):\n",
    "            if max_token_plus_input:\n",
    "                input_token_count= row['input_token_count']\n",
    "                \n",
    "                max_token=input_token_count+model_max_tokens\n",
    "                max_token=int(max_token)\n",
    "            else:\n",
    "                max_token=int(model_max_tokens)\n",
    "                \n",
    "            Experiment_detail= response=structure_answer= correctness = None\n",
    "            Experiment_detail, response, raw_output = LMstudio_playing_openai(message_content=row[messsage_content_column], request_timeout=request_timeout, \n",
    "                                                                                                model_tempreature=model_tempreature, model_max_tokens=max_token)\n",
    "            \n",
    "            return idx, Experiment_detail, response, raw_output\n",
    "    \n",
    "        # Loop through llm list\n",
    "\n",
    "        if llm not in df.columns:\n",
    "            df[llm] = ''\n",
    "        \n",
    "        \n",
    "        i=0\n",
    "        max_index = df.index.max()\n",
    "        for index, row in df.iterrows():\n",
    "            if row[llm] != 'EXTRACTED':\n",
    "\n",
    "                idx, Experiment_detail, response, raw_output = process_row(row, idx=index,model_max_tokens=model_max_tokens,max_token_plus_input=max_token_plus_input)\n",
    "                # Update the DataFrame based on the result\n",
    "                df.at[idx, llm] = 'EXTRACTED'\n",
    "                df.at[idx, f'{llm}_rawresponse'] = str(response) \n",
    "                df.at[idx, f'{llm}_rawoutput'] = str(raw_output)\n",
    "                df.at[idx, f'{llm}_Experiment_detail'] = str(Experiment_detail)\n",
    "                \n",
    "                i+=1\n",
    "                print(f\"idx: {idx}, outcome: {raw_output}\")\n",
    "                \n",
    "                if i==number_of_task_to_save or index == max_index:             \n",
    "                    #save draft\n",
    "                    try:\n",
    "                        df.to_excel(excel_file_path)\n",
    "                        print(f\"Draft excel file saved at {excel_file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in saving temporary excel. Error:   {e}\")\n",
    "                        continue    \n",
    "                    i=0\n",
    "\n",
    "            \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Operation interrupted. Cleaning up...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occured in the handler: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        df = save_and_open_excel(df, excel_file_path, open_at_end)\n",
    "        \n",
    "        \n",
    "        # reset the model experiemnt \n",
    "        LMstudio_playing_openai.has_run_before = False\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path=r\"C:\\Users\\LEGION\\Desktop\\Code4ghaffar\\zero_shot_by_hand.xlsx\"\n",
    "messsage_content_column=\"patient medical hidtory\"\n",
    "#llm=\"phi3-mini4k-fp16\"\n",
    "llm=\"OpenBioLLM-Q8\"\n",
    "\n",
    "openai_api=os.getenv(\"OPENAI_API_KEY\")\n",
    "ground_truth_column=\"Inhospital Mortalit(TRUE)\"\n",
    "open_at_end=True\n",
    "number_of_task_to_save=1\n",
    "request_timeout=70\n",
    "\n",
    "model_tempreature=0.7\n",
    "model_max_tokens=1024\n",
    "max_token_plus_input=False\n",
    "use_seed=True\n",
    "\n",
    "\n",
    "final_df =  handle_LM_studio_general(excel_file_path=excel_file_path,  messsage_content_column=messsage_content_column, llm=llm, openai_api=openai_api, \n",
    "                                                    ground_truth_column=ground_truth_column,\n",
    "                                                    open_at_end=open_at_end,\n",
    "                                                    number_of_task_to_save=number_of_task_to_save,\n",
    "                                                    request_timeout=request_timeout,\n",
    "                                                    \n",
    "                                                    model_tempreature=model_tempreature, model_max_tokens=model_max_tokens, max_token_plus_input=max_token_plus_input,\n",
    "                                                    use_seed=use_seed,\n",
    "                                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lastshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
